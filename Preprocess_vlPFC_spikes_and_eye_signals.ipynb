{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfe/pAjefOieDYZqwLUfPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobellet/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex/blob/main/Preprocess_vlPFC_spikes_and_eye_signals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# This notebook reproduce the preprocessing steps used to align the spike times and eye signal to the time of stimulus onset\n",
        "---\n",
        "For the notebook to download the data from the Figshare repository prior to acceptance of the manuscript you need to insert the private link token mentioned in the \"Code availability\" section of the manuscript.\n",
        "---"
      ],
      "metadata": {
        "id": "2VBTBgIQX7fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "private_link = input('Enter the private link token:')"
      ],
      "metadata": {
        "id": "5EJuREg6X2I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import repository and download data"
      ],
      "metadata": {
        "id": "zH-T1DLCGlih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVfdv2NrpX1U",
        "outputId": "8f0e4cbf-3d4e-4fb6-cd7e-b96b039c3cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading downloads/continuous_signals_spikes_and_events.zip\n",
            "Successfully downloaded downloads/continuous_signals_spikes_and_events.zip.\n",
            "Downloading downloads/myweights\n",
            "Successfully downloaded downloads/myweights.\n"
          ]
        }
      ],
      "source": [
        "files_to_download = ['continuous_signals_spikes_and_events.zip','myweights']\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "IN_COLAB = False\n",
        "IN_KAGGLE = False\n",
        "try:\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        IN_COLAB = True\n",
        "except NameError:\n",
        "    pass\n",
        "if not IN_COLAB:\n",
        "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n",
        "        IN_KAGGLE = True\n",
        "\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    path_to_repo = '/content/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex'\n",
        "elif IN_KAGGLE:\n",
        "    path_to_repo = '/kaggle/working/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex'\n",
        "else:\n",
        "    path_to_repo = 'Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex/'\n",
        "# Only clone if not already present\n",
        "if not os.path.exists(path_to_repo):\n",
        "    os.system(\"git clone https://github.com/jobellet/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex.git \" + path_to_repo)\n",
        "sys.path.append(path_to_repo)\n",
        "\n",
        "\n",
        "from utils.extract_and_download_data import download_files, unzip\n",
        "\n",
        "# Load the file-code mapping\n",
        "\n",
        "download_files(path_to_repo, files_to_download, private_link=private_link)\n",
        "zip_path =  os.path.join('downloads', 'continuous_signals_spikes_and_events.zip')\n",
        "unzip(zip_path,'raw_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViuYwi_wzPwr"
      },
      "source": [
        "### Align Spikes, Eye Traces, and Pupil Data to Stimulus Onset Times  \n",
        "*(0 → 200 ms relative to stimulus onse)*\n",
        "\n",
        "This script loops through each recording-day `.mat` file and generates three aligned outputs per day:\n",
        "\n",
        "1. **Neural spike counts** — `YYYYMMDD_stim_aligned_count.npy`  \n",
        "   *shape (nTrials × 96 channels × 20 bins, 10 ms/bin; 0–200 ms)*  \n",
        "2. **Calibrated eye-position traces** — `YYYYMMDD_eye_pos.npy`  \n",
        "   *shape (nTrials × 1200 samples × 2 axes, 0.6 s at 2 kHz; -300 to 300 ms)*  \n",
        "3. **Baseline-corrected pupil traces** — `YYYYMMDD_pupil_aligned.npy`  \n",
        "   *shape (nTrials × 400 samples, 0.2 s at 2 kHz; 0–200 ms)*  \n",
        "\n",
        "#### Processing Pipeline\n",
        "\n",
        "| Stage | Details |\n",
        "|-------|---------|\n",
        "| **Input discovery** | Scans the working directory for files matching `20*.mat`; skips any that already have all three output `.npy` files. |\n",
        "| **Data loading** | Reads spike times, eye traces (`eye_pos_x`, `eye_pos_y`), pupil size (`eye_pupil`), and stimulus onsets (`stimTime`). |\n",
        "| **Stimulus-time correction** | Shifts each onset **½ video frame earlier** (−8.33 ms at 60 Hz) to compensate for photodiode delay, then converts to sample indices (30 kHz). |\n",
        "| **Eye-position alignment** | Extracts a **200 ms window (0 → +200 ms)** @ 2 kHz, subtracts the median of the neighbouring 15 trials to compensate the slow measurement drift, and maps raw voltages to ° of visual angle via a fixed 2 × 3 affine transform. |\n",
        "| **Spike binning** | Counts spikes in 10 ms bins (**20 bins spanning 0 → +200 ms**) for each of 96 channels using Numba-accelerated search-sorted binning. |\n",
        "| **Pupil alignment** | Extracts the same 200 ms window from `eye_pupil` and subtract the median of the neighbouring 15 trials to compensate the slow measurement drift |\n",
        "| **Saving** | Writes the three outputs with date-encoded filenames so downstream scripts can locate them easily. |\n",
        "| **Parallel execution** | Uses `multiprocessing.Pool` to process multiple recording days in parallel. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn_S9u61J5B-",
        "outputId": "87ae3f0d-a349-4df8-ba40-c923fb1d6ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201022.matProcessing raw_data/20201020.mat\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201022:  45%|████▍     | 1769/3946 [00:08<00:02, 1049.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201023.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201022: 100%|██████████| 3946/3946 [00:10<00:00, 381.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201112.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201023: 100%|██████████| 3963/3963 [00:06<00:00, 646.18it/s]\n",
            "Spikes 20201112:  14%|█▍        | 1375/9873 [00:01<00:08, 973.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201103.mat"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSpikes 20201112:  15%|█▍        | 1474/9873 [00:01<00:08, 962.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201112: 100%|██████████| 9873/9873 [00:10<00:00, 987.17it/s]\n",
            "Spikes 20201103:  48%|████▊     | 3957/8282 [00:04<00:06, 642.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201026.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201103: 100%|██████████| 8282/8282 [00:09<00:00, 858.78it/s] \n",
            "Spikes 20201026:   3%|▎         | 204/6718 [00:00<00:06, 1026.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201027.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201026: 100%|██████████| 6718/6718 [00:05<00:00, 1208.02it/s]\n",
            "Spikes 20201027:  28%|██▊       | 1763/6213 [00:01<00:05, 850.31it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201029.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201027: 100%|██████████| 6213/6213 [00:06<00:00, 930.32it/s]\n",
            "Spikes 20201029:  12%|█▏        | 783/6405 [00:01<00:09, 579.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201110.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201029: 100%|██████████| 6405/6405 [00:07<00:00, 837.75it/s] \n",
            "Spikes 20201110:  13%|█▎        | 1074/8457 [00:00<00:07, 1048.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201109.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201110: 100%|██████████| 8457/8457 [00:07<00:00, 1186.13it/s]\n",
            "Spikes 20201109:  56%|█████▌    | 3600/6433 [00:03<00:03, 890.61it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201030.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201109: 100%|██████████| 6433/6433 [00:06<00:00, 1031.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201104.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201030: 100%|██████████| 6136/6136 [00:07<00:00, 838.12it/s] \n",
            "Spikes 20201104:  29%|██▉       | 2802/9530 [00:02<00:06, 1075.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201106.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201104: 100%|██████████| 9530/9530 [00:07<00:00, 1195.80it/s]\n",
            "Spikes 20201106:  49%|████▉     | 2799/5731 [00:02<00:03, 759.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201102.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201106: 100%|██████████| 5731/5731 [00:06<00:00, 892.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201021.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201021: 100%|██████████| 2365/2365 [00:02<00:00, 1147.91it/s]\n",
            "Spikes 20201102: 100%|██████████| 6903/6903 [00:07<00:00, 918.54it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201028.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201028: 100%|██████████| 4793/4793 [00:01<00:00, 2845.17it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import numba\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import medfilt as movmedian   # simple 1-D median filter\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- geometry helper --------------------------------------------------\n",
        "def transform_points(points):\n",
        "    \"\"\"Affine transform from voltage space to visual-angle space (hard-coded).\"\"\"\n",
        "    T_hard_coded = np.array([[ 6.86341798e-03,  3.92195313e-04, -2.22044166e-16],\n",
        "                             [-1.16678106e-03,  5.64761251e-03, -4.03638942e-16]])\n",
        "    points_h = np.hstack([points, np.ones((points.shape[0], 1))])  # hom. coords\n",
        "    return (points_h @ T_hard_coded.T)[:, :2]\n",
        "\n",
        "# ---------- eye-trace alignment ----------------------------------------------\n",
        "def calibrate_and_align_eye_pos(eye_pos_x, eye_pos_y, stimulus_times):\n",
        "    \"\"\"\n",
        "    Aligns eye position traces in the window -300 to 300 ms relative to stimulus onset.\n",
        "    We take more samples to find end points of microsaccade starting just before the next image onset\n",
        "    We will crop to 200 ms post stimulus onset after\n",
        "    Returns: eye_pos (nTrials × 1200 × 2).\n",
        "    \"\"\"\n",
        "    rate = 2000          # Hz\n",
        "    pre  = -int(0.3 * rate)   # -300 ms\n",
        "    post = int(0.3 * rate)   # -300 ms\n",
        "    baseline_len = 40     #  20 ms for baseline subtraction\n",
        "\n",
        "    stim_idx = (stimulus_times * rate).astype(np.int32)[:, None]\n",
        "    indices  = stim_idx + np.arange(pre, post)             # -300 → +299 ms\n",
        "\n",
        "    X = eye_pos_x[0, indices]\n",
        "    Y = eye_pos_y[0, indices]\n",
        "\n",
        "    # median baseline (20 ms) per trial\n",
        "\n",
        "    X_med = movmedian(np.mean(X[:, -pre-baseline_len:-pre], axis=1), 151).reshape(-1, 1)\n",
        "    Y_med = movmedian(np.mean(Y[:, -pre-baseline_len:-pre], axis=1), 151).reshape(-1, 1)\n",
        "\n",
        "    X_adj = X - X_med\n",
        "    Y_adj = Y - Y_med\n",
        "\n",
        "    pts   = np.column_stack((X_adj.ravel(), Y_adj.ravel()))\n",
        "    out   = transform_points(pts).reshape(X_adj.shape[0], post-pre, 2)\n",
        "\n",
        "    return out\n",
        "\n",
        "# ---------- pupil alignment ---------------------------------------------------\n",
        "def align_pupil_data(eye_pupil, stimulus_times):\n",
        "    \"\"\"\n",
        "    Aligns pupil traces to stimulus onset, keeping only 0–200 ms.\n",
        "    Returns: pupil_aligned (nTrials × 400).\n",
        "    \"\"\"\n",
        "    rate = 2000\n",
        "    post = int(0.2 * rate)   # 400 samples\n",
        "    stim_onset_sample_rate = 30000 # Hz, sample rate of stimulus time stamps\n",
        "\n",
        "    # Replace NaN values with nanmedian of the whole signal\n",
        "    pupil_data = eye_pupil[0].copy() # Create a copy to avoid modifying original data\n",
        "    pupil_nanmedian = np.nanmedian(pupil_data)\n",
        "    pupil_data[np.isnan(pupil_data)] = pupil_nanmedian\n",
        "\n",
        "    # Get pupil value at stimulus onset (convert stimulus time to pupil samples)\n",
        "    stim_onset_pupil_samples = (stimulus_times * rate).astype(int)\n",
        "\n",
        "    # Apply median filter across trials using pupil at stimulus onset\n",
        "    pupil_at_onset = pupil_data[stim_onset_pupil_samples]\n",
        "    pupil_trend = movmedian(pupil_at_onset, 151) # Window of 151 trials\n",
        "\n",
        "    # Align pupil traces\n",
        "    stim_idx = (stimulus_times * rate).astype(np.int32)[:, None]\n",
        "    indices  = stim_idx + np.arange(0, post)\n",
        "\n",
        "    pupil_aligned = pupil_data[indices]\n",
        "\n",
        "    # Subtract the trial-wise trend from the aligned pupil data\n",
        "    pupil_detrended = pupil_aligned - pupil_trend[:, None]\n",
        "\n",
        "    return pupil_detrended\n",
        "\n",
        "# ---------- spike-count binning ----------------------------------------------\n",
        "TIME_WINDOW = 10 * 30        # 300 samples = 10 ms @ 30 kHz\n",
        "NUM_BINS    = 20             # 0–200 ms\n",
        "@numba.njit\n",
        "def compute_channel_counts(spike_times, edges):\n",
        "    return np.diff(np.searchsorted(spike_times, edges)).astype(np.int32)\n",
        "\n",
        "def get_spike_count(spike, t):\n",
        "    \"\"\"\n",
        "    Compute 10 ms spike counts from 0–200 ms after stimulus onset.\n",
        "    Returns: (96 × 20) int32 array.\n",
        "    \"\"\"\n",
        "    edges = np.arange(t, t + (NUM_BINS + 1) * TIME_WINDOW, TIME_WINDOW)\n",
        "    counts = np.zeros((96, NUM_BINS), dtype=np.int32)\n",
        "    for ch in range(96):\n",
        "        st = spike[0][ch][0, :]   # 1-D sorted spike times (samples)\n",
        "        counts[ch, :] = compute_channel_counts(st, edges)\n",
        "    return counts\n",
        "\n",
        "# ---------- per-file processing ----------------------------------------------\n",
        "def process_file(mat_file):\n",
        "    print(f\"Processing {mat_file}\")\n",
        "    base = os.path.splitext(os.path.basename(mat_file))[0]\n",
        "    f_spk, f_eye, f_pup = [f\"aligned_data/{base}_{suffix}.npy\" for suffix in\n",
        "                           (\"stim_aligned_count\", \"eye_pos\", \"pupil_aligned\")]\n",
        "    if all(os.path.exists(f) for f in (f_spk, f_eye, f_pup)):\n",
        "        print(\"  → outputs already exist, skipping.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        m = loadmat(mat_file)\n",
        "        stimTime = m[\"stimTime\"][0]       # seconds\n",
        "        spike    = m[\"spike\"]\n",
        "        eye_x    = m[\"eye_pos_x\"]\n",
        "        eye_y    = m[\"eye_pos_y\"]\n",
        "        eye_pup  = m[\"eye_pupil\"]\n",
        "\n",
        "        # Constant photodiode correction (–8.33 ms) to prevent underestimating response delay in vlPFC due screen rolling shutter effect\n",
        "        stim_corr = stimTime - (1 / 60) / 2\n",
        "\n",
        "        eye_pos = calibrate_and_align_eye_pos(eye_x, eye_y, stim_corr)\n",
        "        pupil      = align_pupil_data(eye_pup, stim_corr)\n",
        "\n",
        "        stim_counts = np.zeros((len(stim_corr), 96, NUM_BINS), dtype=np.int32)\n",
        "        stim_corr_samples = (stim_corr * 30000).astype(np.int32)\n",
        "\n",
        "        for s, t in tqdm(enumerate(stim_corr_samples),\n",
        "                         total=len(stim_corr_samples),\n",
        "                         desc=f\"Spikes {base}\"):\n",
        "            stim_counts[s] = get_spike_count(spike, t)\n",
        "\n",
        "        np.save(f_spk, stim_counts)\n",
        "        np.save(f_eye, eye_pos)\n",
        "        np.save(f_pup, pupil)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mat_file}: {e}\")\n",
        "\n",
        "# ---------- main entry-point --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs('aligned_data/', exist_ok=True)\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        pool.map(process_file, glob(os.path.join('raw_data',\"20*.mat\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcU6kDCq0btS"
      },
      "source": [
        "### Aggregate Daily Outputs & Down-sample Eye/Pupil Signals to match spike count 10 ms binning\n",
        "\n",
        "This script collates the per-day `.npy` files created above into single, experiment-wide arrays and a tidy metadata table:\n",
        "\n",
        "1. **Spike counts** → `aggregated_spike_counts.npy`  \n",
        "   *shape (nTotalTrials × 96 × 20)*  \n",
        "2. **Eye position** (down-sampled to 100 Hz) → `aggregated_eye_pos.npy`  \n",
        "   *shape (nTotalTrials × 20 × 2)*  \n",
        "3. **Pupil traces** (down-sampled to 100 Hz) → `aggregated_pupil.npy`  \n",
        "   *shape (nTotalTrials × 20)*  \n",
        "4. **Trial-wise metadata CSV** → `metadata.csv`  \n",
        "   Columns: global trial index, recording date, `stimId`, original `stimTime`.\n",
        "\n",
        "#### Key Steps\n",
        "\n",
        "1. **File matching** – Identifies every `.mat` file beginning with `20`. For each, it expects the three aligned outputs generated by Script 1; files lacking them are skipped with a warning.  \n",
        "2. **Down-sampling** – Eye and pupil traces (2 kHz) are reduced to 100 Hz (20×) using `downsample_array`, preserving the **0 → 200 ms** window as **20 samples**.  \n",
        "3. **Data stacking** – Trial-level arrays are appended to running lists; after all days are processed, `np.concatenate` merges them along the trial axis.  \n",
        "4. **Metadata assembly** – A `pandas` DataFrame stores per-trial identifiers and is written to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad975f8f-0ee9-49cf-9a45-fa77f1b35d10",
        "id": "x9lz3KMZ0btS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: 20201022\n",
            "Processing file: 20201112\n",
            "Processing file: 20201020\n",
            "Processing file: 20201023\n",
            "Processing file: 20201103\n",
            "Processing file: 20201027\n",
            "Processing file: 20201026\n",
            "Processing file: 20201029\n",
            "Processing file: 20201110\n",
            "Processing file: 20201030\n",
            "Processing file: 20201109\n",
            "Processing file: 20201104\n",
            "Processing file: 20201106\n",
            "Processing file: 20201021\n",
            "Processing file: 20201102\n",
            "Processing file: 20201028\n",
            "Data aggregation complete!\n",
            "Total trials aggregated: 97806\n",
            "Metadata shape: (97806, 4)\n",
            "Spike count array shape: (97806, 96, 20)\n",
            "Eye position array shape (100 Hz): (97806, 20, 2)\n",
            "Eye position array shape (250 Hz): (97806, 150, 2)\n",
            "Pupil array shape: (97806, 20)\n"
          ]
        }
      ],
      "source": [
        "def downsample_array(array, factor, axis):\n",
        "    \"\"\"\n",
        "    Downsample a NumPy array along a specified axis by a given factor.\n",
        "\n",
        "    Parameters:\n",
        "    - array: np.ndarray, input data of any shape.\n",
        "    - factor: int, downsampling factor (keep every `factor`-th element). For instance factor = 2 halfs the sampling rate.\n",
        "    - axis: int, axis along which to downsample.\n",
        "\n",
        "    Returns:\n",
        "    - downsampled_array: np.ndarray, array downsampled along the specified axis.\n",
        "    \"\"\"\n",
        "    # Build a slicing object: slice(None) for all axes, except step-slice on the target axis\n",
        "    slicer = [slice(None)] * array.ndim\n",
        "    slicer[axis] = slice(None, None, factor)\n",
        "    return array[tuple(slicer)]\n",
        "\n",
        "\n",
        "# List all .mat files starting with \"20\" in the current directory\n",
        "mat_files = glob(os.path.join('raw_data','20*.mat'))\n",
        "\n",
        "# Prepare lists to accumulate data from all files\n",
        "all_metadata = []\n",
        "all_spike_counts = []\n",
        "all_eye_pos = []\n",
        "all_pupil = []\n",
        "all_eye_pos_250 = [] # New list for 250 Hz eye position\n",
        "\n",
        "# Running counter for total trials\n",
        "total_trials = 0\n",
        "\n",
        "for mat_file in mat_files:\n",
        "    base = os.path.splitext(os.path.basename(mat_file))[0]\n",
        "    print(f\"Processing file: {base}\")\n",
        "\n",
        "    spike_file = f\"aligned_data/{base}_stim_aligned_count.npy\"\n",
        "    eye_file   = f\"aligned_data/{base}_eye_pos.npy\"\n",
        "    pupil_file = f\"aligned_data/{base}_pupil_aligned.npy\"\n",
        "\n",
        "    if not (os.path.exists(mat_file) and os.path.exists(spike_file)\n",
        "            and os.path.exists(eye_file) and os.path.exists(pupil_file)):\n",
        "        print(f\"Skipping file {base}: required files not found.\")\n",
        "        continue\n",
        "\n",
        "    mat_data = loadmat(mat_file)\n",
        "    stimId   = mat_data['stimId'][0] if 'stimId' in mat_data else None\n",
        "    stimTime = mat_data['stimTime'][0] if 'stimTime' in mat_data else None\n",
        "\n",
        "    recording_date = base\n",
        "\n",
        "    stim_aligned_count = np.load(spike_file)  # shape: (nTrials, 96, 20)\n",
        "    eye_pos            = np.load(eye_file)    # shape: (nTrials, 400, 2)\n",
        "    pupil_aligned      = np.load(pupil_file)  # shape: (nTrials, 400)\n",
        "\n",
        "    nTrials = stim_aligned_count.shape[0]\n",
        "\n",
        "    # Downsample from 2 kHz → 100 Hz (factor 20): 1200 → 60 samples\n",
        "\n",
        "    eye_pos_downsampled   = downsample_array(eye_pos, factor=20, axis=1)[:,30:50]    # (nTrials, 20, 2)\n",
        "    pupil_downsampled     = downsample_array(pupil_aligned, factor=20, axis=1)  # (nTrials, 20)\n",
        "\n",
        "    # Downsample from 2 kHz → 250 Hz (factor 8): 1200 → 150 samples\n",
        "    eye_pos_downsampled_250 = downsample_array(eye_pos, factor=8, axis=1) # (nTrials, 150, 2)\n",
        "\n",
        "    for i in range(nTrials):\n",
        "        all_metadata.append({\n",
        "            'trial_index_global': total_trials + i,\n",
        "            'recording_date': recording_date,\n",
        "            'stimId': stimId[i] if stimId is not None else None,\n",
        "            'time_of_stimulus': stimTime[i] if stimTime is not None else None\n",
        "        })\n",
        "\n",
        "    all_spike_counts.append(stim_aligned_count.astype(np.int8))\n",
        "    all_eye_pos.append(eye_pos_downsampled.astype(np.float32))\n",
        "    all_pupil.append(pupil_downsampled.astype(np.float32))\n",
        "    all_eye_pos_250.append(eye_pos_downsampled_250.astype(np.float32)) # Append 250 Hz data\n",
        "\n",
        "    total_trials += nTrials\n",
        "\n",
        "## Final concatenation\n",
        "all_spike_counts = np.concatenate(all_spike_counts, axis=0)  # (nTotalTrials, 96, 20)\n",
        "all_eye_pos      = np.concatenate(all_eye_pos, axis=0)       # (nTotalTrials, 20, 2)\n",
        "all_pupil        = np.concatenate(all_pupil, axis=0)         # (nTotalTrials, 20)\n",
        "all_eye_pos_250  = np.concatenate(all_eye_pos_250, axis=0)   # (nTotalTrials, 150, 2) # Concatenate 250 Hz data\n",
        "\n",
        "metadata_df = pd.DataFrame(all_metadata)\n",
        "\n",
        "# Save outputs\n",
        "metadata_df.to_csv(\"recording_metadata.csv\", index=False)\n",
        "np.save(\"aggregated_spike_counts.npy\", all_spike_counts)\n",
        "np.save(\"aggregated_eye_pos.npy\", all_eye_pos)\n",
        "np.save(\"aggregated_pupil.npy\", all_pupil)\n",
        "np.save(\"aggregated_eye_pos_250.npy\", all_eye_pos_250) # Save 250 Hz data\n",
        "\n",
        "# Sanity print\n",
        "print(\"Data aggregation complete!\")\n",
        "print(f\"Total trials aggregated: {total_trials}\")\n",
        "print(f\"Metadata shape: {metadata_df.shape}\")\n",
        "print(f\"Spike count array shape: {all_spike_counts.shape}\")\n",
        "print(f\"Eye position array shape (100 Hz): {all_eye_pos.shape}\")\n",
        "print(f\"Eye position array shape (250 Hz): {all_eye_pos_250.shape}\") # Print shape of 250 Hz data\n",
        "print(f\"Pupil array shape: {all_pupil.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detect microsaccades in the 250 Hz eye signal and save a microsaccade metadata"
      ],
      "metadata": {
        "id": "uXpIOpgfEmFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_eye_pos250 = np.load(\"aggregated_eye_pos_250.npy\")\n",
        "X = aggregated_eye_pos250[:,:,0].clip(-10,10)\n",
        "Y = aggregated_eye_pos250[:,:,1].clip(-10,10)\n",
        "\n",
        "# download U'n'Eye\n",
        "!git clone https://github.com/berenslab/uneye.git\n",
        "\n",
        "Path_to_UnEye_folder = '/content/uneye/'\n",
        "# go to the uneye folder\n",
        "import os\n",
        "os.chdir(Path_to_UnEye_folder)\n",
        "from os.path import exists\n",
        "import uneye\n",
        "# move the file myweights to /uneye/training\n",
        "shutil.move('/content/downloads/myweights', '/content/uneye/training')\n",
        "def calculate_velocity(X, Y, dt=1):\n",
        "    \"\"\" Calculate velocity from position data \"\"\"\n",
        "    Vx = np.diff(X, axis=1, append=X[:, -1:]) / dt\n",
        "    Vy = np.diff(Y, axis=1, append=Y[:, -1:]) / dt\n",
        "    return np.sqrt(Vx**2 + Vy**2)\n",
        "\n",
        "\n",
        "def find_saccades(L):\n",
        "    \"\"\"Find start and end indices of saccades in a faster, more vectorized way.\n",
        "\n",
        "    Each saccade is represented by a tuple:\n",
        "      (trial, start_time, end_time)\n",
        "    If a saccade has not ended, its end is set to n_time - 1.\n",
        "    \"\"\"\n",
        "    n_trials, n_time = L.shape\n",
        "    # Compute the difference along time axis after converting to int8.\n",
        "    diffL = np.diff(L.astype(np.int8), axis=1, prepend=0)\n",
        "\n",
        "    # Identify start and end events.\n",
        "    start_rows, start_cols = np.where(diffL == 1)\n",
        "    end_rows, end_cols = np.where(diffL == -1)\n",
        "\n",
        "    saccades = []\n",
        "\n",
        "    # Process each trial separately.\n",
        "    for trial in range(n_trials):\n",
        "        # Extract start and end times for the current trial.\n",
        "        trial_starts = start_cols[start_rows == trial]\n",
        "        trial_ends = end_cols[end_rows == trial]\n",
        "\n",
        "        # If no start events, skip the trial.\n",
        "        if trial_starts.size == 0:\n",
        "            continue\n",
        "\n",
        "        # For each start, find the insertion point in the sorted trial_ends array.\n",
        "        indices = np.searchsorted(trial_ends, trial_starts, side='right')\n",
        "\n",
        "        # Prepare an array to hold the end times.\n",
        "        ends_for_trial = np.empty(trial_starts.shape, dtype=int)\n",
        "        valid = indices < trial_ends.size\n",
        "\n",
        "        # For valid indices, set the corresponding trial_ends.\n",
        "        ends_for_trial[valid] = trial_ends[indices[valid]]\n",
        "        # For indices out of bounds, assign n_time - 1.\n",
        "        ends_for_trial[~valid] = n_time - 1\n",
        "\n",
        "        # Append the results as tuples.\n",
        "        saccades.extend(zip(np.full(trial_starts.shape, trial), trial_starts, ends_for_trial))\n",
        "\n",
        "    return np.array(saccades, dtype='object')\n",
        "\n",
        "def median_position(X, Y, trials,times, offset):\n",
        "    \"\"\" Compute median position around given indices with an offset \"\"\"\n",
        "    positions = []\n",
        "\n",
        "    for i, t in zip(trials, times):\n",
        "        t_start, t_end = max(t + offset[0], 0), min(t + offset[1], X.shape[1])\n",
        "        median_x = np.median(X[i, t_start:t_end])\n",
        "        median_y = np.median(Y[i, t_start:t_end])\n",
        "        positions.append((median_x, median_y))\n",
        "    return positions\n",
        "\n",
        "def save_data(df, output_folder, filename):\n",
        "    \"\"\" Saves the DataFrame in CSV and MATLAB .mat formats. \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    pkl_path = os.path.join(output_folder, f\"{filename}.pkl\")\n",
        "    df.to_pickle(pkl_path)\n",
        "min_sacc_dur = 10 #minimum saccade duration in ms\n",
        "min_sacc_dist = 10 #minimum saccade distance in ms\n",
        "sampfreq = 250 #Hz\n",
        "\n",
        "weights_name = 'myweights'\n",
        "\n",
        "# create model\n",
        "model = uneye.DNN(weights_name=weights_name,\n",
        "                 sampfreq=sampfreq,\n",
        "                 min_sacc_dur=min_sacc_dur,\n",
        "                 min_sacc_dist=min_sacc_dist,)\n",
        "\n",
        "# Label data\n",
        "L,P = model.predict(X,Y)\n",
        "\n",
        "velocity = calculate_velocity(X, Y)\n",
        "\n",
        "saccades_bounds = find_saccades(L)\n",
        "trials,saccade_starts, saccade_ends = saccades_bounds[:,0],saccades_bounds[:,1],saccades_bounds[:,2]\n",
        "start_positions = median_position(X, Y, trials,saccade_starts, offset=(-10, 0))\n",
        "end_positions = median_position(X, Y, trials,saccade_ends, offset=(0, 10))\n",
        "\n",
        "saccades = []\n",
        "for i,t_start, t_end, start_pos, end_pos in zip(trials,saccade_starts, saccade_ends, start_positions, end_positions):\n",
        "\n",
        "    duration = t_end - t_start\n",
        "    amplitude = np.sqrt(np.sum((np.array(end_pos)-np.array(start_pos))**2))\n",
        "    peak_velocity_time = np.argmax(velocity[i, t_start:t_end]) + t_start\n",
        "    peak_velocity = velocity[i, peak_velocity_time]\n",
        "    delta_x = end_pos[0] - start_pos[0]\n",
        "    delta_y = end_pos[1] - start_pos[1]\n",
        "    polar_angle = np.arctan2(delta_y, delta_x) # Calculate polar angle in radians\n",
        "\n",
        "    saccades.append({\n",
        "        \"Trial\": i,\n",
        "        \"Start_Time\": (t_start/250)-.3, # signal starts .3 seconds before stimulus onset\n",
        "        \"Stop_Time\": (t_end/250)-.3,\n",
        "        \"Duration\": duration/250,\n",
        "        \"Start_X\": start_pos[0],\n",
        "        \"Start_Y\": start_pos[1],\n",
        "        \"Stop_X\": end_pos[0],\n",
        "        \"Stop_Y\": end_pos[1],\n",
        "        \"Amplitude\": amplitude,\n",
        "        \"Peak_Velocity\": peak_velocity/250,\n",
        "        \"Time_of_Peak_Velocity\": (peak_velocity_time - t_start)/250,\n",
        "        \"polar_angle\": polar_angle # Add polar angle to the dictionary\n",
        "    })\n",
        "\n",
        "df_saccades = pd.DataFrame(saccades)\n",
        "df_saccades = df_saccades[df_saccades.Start_Time<=.2] # remove saccades starting after next stimulus onset\n",
        "df_saccades = df_saccades[df_saccades.Stop_Time>=0] # remove saccades ending prior to stimulus onset\n",
        "save_data(df_saccades, '/content', 'saccade_data')\n",
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "RfRY_4vIF7Re",
        "outputId": "a4e8f816-8f1b-4890-c732-6bbc0591bf61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uneye'...\n",
            "remote: Enumerating objects: 990, done.\u001b[K\n",
            "remote: Total 990 (delta 0), reused 0 (delta 0), pack-reused 990 (from 1)\u001b[K\n",
            "Receiving objects: 100% (990/990), 169.38 MiB | 17.71 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n",
            "Updating files: 100% (489/489), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o-jVgsXz0sn"
      },
      "source": [
        "### Split Odd- vs. Even-Day Trials & Stimulus-Average Responses\n",
        "\n",
        "This step takes the aggregated trial-level arrays and produces\n",
        "stimulus-wise averages for **odd** and **even recording days** separately.\n",
        "\n",
        "#### Workflow\n",
        "\n",
        "| Stage | Action | Detail |\n",
        "|-------|--------|--------|\n",
        "| **1. Load aggregated data** | `recording_metadata.csv`, `aggregated_spike_counts.npy`, `aggregated_eye_pos.npy`, `aggregated_pupil.npy` | Shapes: (nTrials × 96 × 20), (nTrials × 20 × 2), (nTrials × 20) |\n",
        "| **2. Temporal sort** | Sort trials by `recording_date` + `time_of_stimulus` | Guarantees chronological order within a day |\n",
        "| **3. Session index** | Map each `recording_date` → integer index | Needed to tag even vs odd days |\n",
        "| **4. Trial-exclusion masks** | *a)* Drop any stimulus that repeats within the **previous two trials** of the same day<br>*b)* Require fixation radius < 2° for the **entire 0–200 ms window** | Reduces adaptation effects and eye-movement artefacts |\n",
        "| **5. Session-specific outlier filter** | Within each day, discard trials whose **mean firing rate** falls outside the 0.1-th–99.9-th percentile range | Removes electrical artefacts or silent blocks |\n",
        "| **6. Session baseline shift** | For every session, subtract the channel-wise mean firing rate (averaged over time bins) before averaging | Aligns days with different baselines |\n",
        "| **7. Odd/Even split & averaging** | For each `stimId`, compute the mean of baseline-shifted trials **separately** for odd-indexed sessions and even-indexed sessions | For each channel, outliers are prevented by clipping rates between 1st and 99th percentiles |\n",
        "| **8. Eye & pupil means** | Simultaneously average eye-position (20 samples @ 100 Hz) and pupil traces for odd vs even groups | For control analysis |\n",
        "| **9. Save outputs** | ```\n",
        "Spike_count_even_sessions.npy   # (nStim × 96 × 20)\n",
        "Spike_count_odd_sessions.npy\n",
        "Mean_eye_position_even_sessions.npy   # (nStim × 20 × 2)\n",
        "Mean_eye_position_odd_sessions.npy\n",
        "Mean_pupil_size_even_sessions.npy     # (nStim × 20)\n",
        "Mean_pupil_size_odd_sessions.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TvVf5u0KBXDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0ec22c-9788-4362-f63b-cecf549a2269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Computing session-based shifted averages per stimulus, excluding bad trials...\n",
            "Computing mean eye position and pupil size per stimulus for even and odd sessions...\n",
            "Saving outputs...\n",
            "Saved df_saccades_even.pkl\n",
            "Saved df_saccades_odd.pkl\n",
            "Done! Session-split shifted arrays and mean signals saved.\n"
          ]
        }
      ],
      "source": [
        "metadata_csv = \"recording_metadata.csv\"\n",
        "spike_file = \"aggregated_spike_counts.npy\"\n",
        "eye_file = \"aggregated_eye_pos.npy\"\n",
        "pupil_file = \"aggregated_pupil.npy\"\n",
        "n_stim_ids = 3200\n",
        "output_prefix = \"\"\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# 1) Load data\n",
        "metadata_df = pd.read_csv(metadata_csv)\n",
        "all_spike_counts = np.load(spike_file)   # shape: (nTrials, 96, 20)\n",
        "all_eye_pos = np.load(eye_file)          # shape: (nTrials, 20, 2)\n",
        "all_pupil = np.load(pupil_file)          # shape: (nTrials, 20)\n",
        "\n",
        "# 2) Sort trials by [recording_date, time_of_stimulus]\n",
        "if \"time_of_stimulus\" not in metadata_df.columns:\n",
        "    raise ValueError(\"Metadata must have a 'time_of_stimulus' column for sorting.\")\n",
        "\n",
        "metadata_df[\"sort_key\"] = (\n",
        "    metadata_df[\"recording_date\"].astype(str) + \"_\" +\n",
        "    metadata_df[\"time_of_stimulus\"].astype(str)\n",
        ")\n",
        "sorted_indices = np.argsort(metadata_df[\"sort_key\"].values)\n",
        "\n",
        "metadata_df = metadata_df.iloc[sorted_indices].reset_index(drop=True)\n",
        "all_spike_counts = all_spike_counts[sorted_indices]\n",
        "all_eye_pos = all_eye_pos[sorted_indices]\n",
        "all_pupil = all_pupil[sorted_indices]\n",
        "\n",
        "# 3) Compute day indices (sessions) for each trial\n",
        "unique_days = np.sort(metadata_df[\"recording_date\"].unique())\n",
        "day_index_map = {day: i for i, day in enumerate(unique_days)}\n",
        "day_indices = metadata_df[\"recording_date\"].map(day_index_map).values\n",
        "\n",
        "# 4) Identify non-repeating stimuli\n",
        "nTrials = len(metadata_df)\n",
        "non_repeating_stimuli = np.ones(nTrials, dtype=bool)\n",
        "stim_ids_series = metadata_df[\"stimId\"].values\n",
        "\n",
        "for i in range(1, nTrials):\n",
        "    start_idx = max(0, i - 2)\n",
        "    if np.any(stim_ids_series[start_idx:i] == stim_ids_series[i]):\n",
        "        non_repeating_stimuli[i] = False\n",
        "\n",
        "# 5) Identify in-fixation-window trials (within 2° radius during all 20 samples)\n",
        "radius = np.sqrt(all_eye_pos[..., 0]**2 + all_eye_pos[..., 1]**2)  # (nTrials, 20)\n",
        "in_fix_window_trials = np.mean(radius < 2, axis=1) == 1\n",
        "\n",
        "# 6) Apply validity mask and filter outliers by session\n",
        "valid_mask = non_repeating_stimuli & in_fix_window_trials\n",
        "sessions = np.unique(day_indices)\n",
        "\n",
        "for s in sessions:\n",
        "    session_indices = np.where(day_indices == s)[0]\n",
        "    valid_session_indices = session_indices[valid_mask[session_indices]]\n",
        "    if valid_session_indices.size > 0:\n",
        "        trial_mean_fr = np.mean(all_spike_counts[valid_session_indices], axis=(1, 2))\n",
        "        lo = np.percentile(trial_mean_fr, 0.1)\n",
        "        hi = np.percentile(trial_mean_fr, 99.9)\n",
        "        condition = (trial_mean_fr > lo) & (trial_mean_fr < hi)\n",
        "        valid_mask[valid_session_indices] = condition & valid_mask[valid_session_indices]\n",
        "\n",
        "# 7) Compute per-session mean for baseline shift\n",
        "session_means = {}\n",
        "for s in sessions:\n",
        "    valid_trials = np.where((day_indices == s) & valid_mask)[0]\n",
        "    if valid_trials.size > 0:\n",
        "        session_means[s] = np.mean(all_spike_counts[valid_trials], axis=(0, 2))  # (96,)\n",
        "    else:\n",
        "        session_means[s] = np.zeros(96)\n",
        "\n",
        "# 8) Initialize output arrays for spike counts\n",
        "Spike_count_even = np.full((n_stim_ids, 96, 20), np.nan, dtype=np.float32)\n",
        "Spike_count_odd  = np.full((n_stim_ids, 96, 20), np.nan, dtype=np.float32)\n",
        "\n",
        "# Initialize output arrays for spike counts (microsaccade-free)\n",
        "Spike_count_even_ms_free = np.full((n_stim_ids, 96, 20), np.nan, dtype=np.float32)\n",
        "Spike_count_odd_ms_free  = np.full((n_stim_ids, 96, 20), np.nan, dtype=np.float32)\n",
        "\n",
        "\n",
        "print(\"Computing session-based shifted averages per stimulus, excluding bad trials...\")\n",
        "\n",
        "# Loop over each stimulus\n",
        "for stim_id in range(n_stim_ids):\n",
        "    stim_indices = np.where((stim_ids_series == stim_id) & valid_mask)[0]\n",
        "    if stim_indices.size == 0:\n",
        "        continue\n",
        "\n",
        "    even_indices = stim_indices[(day_indices[stim_indices] % 2) == 0]\n",
        "    odd_indices  = stim_indices[(day_indices[stim_indices] % 2) == 1]\n",
        "\n",
        "    shifted_trials_even = [\n",
        "        all_spike_counts[idx] - session_means[day_indices[idx]][:, None]\n",
        "        for idx in even_indices\n",
        "    ]\n",
        "    if shifted_trials_even:\n",
        "        Spike_count_even[stim_id] = np.mean(shifted_trials_even, axis=0)\n",
        "\n",
        "    shifted_trials_odd = [\n",
        "        all_spike_counts[idx] - session_means[day_indices[idx]][:, None]\n",
        "        for idx in odd_indices\n",
        "    ]\n",
        "    if shifted_trials_odd:\n",
        "        Spike_count_odd[stim_id] = np.mean(shifted_trials_odd, axis=0)\n",
        "\n",
        "\n",
        "# Compute microsaccade-free valid mask\n",
        "\n",
        "trials_with_saccades = df_saccades['Trial'].unique()\n",
        "valid_mask_ms_free = valid_mask.copy()\n",
        "valid_mask_ms_free[trials_with_saccades] = False\n",
        "\n",
        "# More stingent filter in case of eye drifts or undetected microsaccade\n",
        "eye_deviation = np.sqrt(np.sum((all_eye_pos[:,:2,:].mean(1)-all_eye_pos[:,-2:,:].mean(1))**2,1))\n",
        "deviation_threshold = np.percentile(eye_deviation[valid_mask_ms_free], 90)\n",
        "valid_mask_ms_free = valid_mask_ms_free & (eye_deviation<deviation_threshold)\n",
        "\n",
        "# Loop over each stimulus for microsaccade-free data\n",
        "for stim_id in range(n_stim_ids):\n",
        "    stim_indices_ms_free = np.where((stim_ids_series == stim_id) & valid_mask_ms_free)[0]\n",
        "    if stim_indices_ms_free.size == 0:\n",
        "        continue\n",
        "\n",
        "    even_indices_ms_free = stim_indices_ms_free[(day_indices[stim_indices_ms_free] % 2) == 0]\n",
        "    odd_indices_ms_free  = stim_indices_ms_free[(day_indices[stim_indices_ms_free] % 2) == 1]\n",
        "\n",
        "    shifted_trials_even_ms_free = [\n",
        "        all_spike_counts[idx] - session_means[day_indices[idx]][:, None]\n",
        "        for idx in even_indices_ms_free\n",
        "    ]\n",
        "    if shifted_trials_even_ms_free:\n",
        "        Spike_count_even_ms_free[stim_id] = np.mean(shifted_trials_even_ms_free, axis=0)\n",
        "\n",
        "    shifted_trials_odd_ms_free = [\n",
        "        all_spike_counts[idx] - session_means[day_indices[idx]][:, None]\n",
        "        for idx in odd_indices_ms_free\n",
        "    ]\n",
        "    if shifted_trials_odd_ms_free:\n",
        "        Spike_count_odd_ms_free[stim_id] = np.mean(shifted_trials_odd_ms_free, axis=0)\n",
        "\n",
        "# 9) Clip spike counts channel-wise between 1st and 99th percentiles\n",
        "def clip_spike_count(spike_count):\n",
        "    # Filter out NaNs before calculating percentiles\n",
        "    valid_data = spike_count[~np.isnan(spike_count)]\n",
        "    if valid_data.size == 0:\n",
        "        return spike_count # Return original if no valid data\n",
        "\n",
        "    rate_channel = spike_count.swapaxes(0, 1).reshape(96, -1)\n",
        "    # Calculate percentiles only on non-NaN values\n",
        "    low = np.array([np.nanpercentile(rate_channel[i, ~np.isnan(rate_channel[i])], 1) if not np.all(np.isnan(rate_channel[i])) else np.nan for i in range(rate_channel.shape[0])])\n",
        "    high = np.array([np.nanpercentile(rate_channel[i, ~np.isnan(rate_channel[i])], 99) if not np.all(np.isnan(rate_channel[i])) else np.nan for i in range(rate_channel.shape[0])])\n",
        "\n",
        "    clipped = np.clip(rate_channel, low[:, None], high[:, None])\n",
        "    return clipped.reshape(spike_count.swapaxes(0, 1).shape).swapaxes(0, 1)\n",
        "\n",
        "\n",
        "Spike_count_even = clip_spike_count(Spike_count_even)\n",
        "Spike_count_odd = clip_spike_count(Spike_count_odd)\n",
        "\n",
        "if 'df_saccades' in locals() and not df_saccades.empty:\n",
        "    Spike_count_even_ms_free = clip_spike_count(Spike_count_even_ms_free)\n",
        "    Spike_count_odd_ms_free = clip_spike_count(Spike_count_odd_ms_free)\n",
        "\n",
        "\n",
        "# 10) Initialize output arrays for eye and pupil signals\n",
        "Mean_eye_position_even = np.full((n_stim_ids, 20, 2), np.nan, dtype=np.float32)\n",
        "Mean_eye_position_odd  = np.full((n_stim_ids, 20, 2), np.nan, dtype=np.float32)\n",
        "Mean_pupil_size_even   = np.full((n_stim_ids, 20), np.nan, dtype=np.float32)\n",
        "Mean_pupil_size_odd    = np.full((n_stim_ids, 20), np.nan, dtype=np.float32)\n",
        "\n",
        "# Initialize output arrays for eye and pupil signals (microsaccade-free)\n",
        "Mean_eye_position_even_ms_free = np.full((n_stim_ids, 20, 2), np.nan, dtype=np.float32)\n",
        "Mean_eye_position_odd_ms_free  = np.full((n_stim_ids, 20, 2), np.nan, dtype=np.float32)\n",
        "Mean_pupil_size_even_ms_free   = np.full((n_stim_ids, 20), np.nan, dtype=np.float32)\n",
        "Mean_pupil_size_odd_ms_free    = np.full((n_stim_ids, 20), np.nan, dtype=np.float32)\n",
        "\n",
        "\n",
        "print(\"Computing mean eye position and pupil size per stimulus for even and odd sessions...\")\n",
        "\n",
        "# Loop over each stimulus ID for eye/pupil averaging\n",
        "for stim_id in range(n_stim_ids):\n",
        "    stim_indices = np.where((stim_ids_series == stim_id) & valid_mask)[0]\n",
        "    if stim_indices.size == 0:\n",
        "        continue\n",
        "\n",
        "    even_indices = stim_indices[(day_indices[stim_indices] % 2) == 0]\n",
        "    odd_indices  = stim_indices[(day_indices[stim_indices] % 2) == 1]\n",
        "\n",
        "    if even_indices.size > 0:\n",
        "        Mean_eye_position_even[stim_id] = np.mean(all_eye_pos[even_indices], axis=0)\n",
        "        Mean_pupil_size_even[stim_id] = np.mean(all_pupil[even_indices], axis=0)\n",
        "    if odd_indices.size > 0:\n",
        "        Mean_eye_position_odd[stim_id] = np.mean(all_eye_pos[odd_indices], axis=0)\n",
        "        Mean_pupil_size_odd[stim_id] = np.mean(all_pupil[odd_indices], axis=0)\n",
        "\n",
        "    # Microsaccade-free averaging\n",
        "    if 'df_saccades' in locals() and not df_saccades.empty:\n",
        "        stim_indices_ms_free = np.where((stim_ids_series == stim_id) & valid_mask_ms_free)[0]\n",
        "        if stim_indices_ms_free.size > 0:\n",
        "            even_indices_ms_free = stim_indices_ms_free[(day_indices[stim_indices_ms_free] % 2) == 0]\n",
        "            odd_indices_ms_free  = stim_indices_ms_free[(day_indices[stim_indices_ms_free] % 2) == 1]\n",
        "\n",
        "            if even_indices_ms_free.size > 0:\n",
        "                Mean_eye_position_even_ms_free[stim_id] = np.mean(all_eye_pos[even_indices_ms_free], axis=0)\n",
        "                Mean_pupil_size_even_ms_free[stim_id] = np.mean(all_pupil[even_indices_ms_free], axis=0)\n",
        "            if odd_indices_ms_free.size > 0:\n",
        "                Mean_eye_position_odd_ms_free[stim_id] = np.mean(all_eye_pos[odd_indices_ms_free], axis=0)\n",
        "                Mean_pupil_size_odd_ms_free[stim_id] = np.mean(all_pupil[odd_indices_ms_free], axis=0)\n",
        "\n",
        "\n",
        "# Add splitting of df_saccades\n",
        "if 'df_saccades' in locals() and not df_saccades.empty:\n",
        "    df_saccades_even = df_saccades[df_saccades['Trial'].isin(np.where((day_indices % 2) == 0)[0])].copy()\n",
        "    df_saccades_odd = df_saccades[df_saccades['Trial'].isin(np.where((day_indices % 2) == 1)[0])].copy()\n",
        "\n",
        "    # Add 'stimId' column to df_saccades_even and df_saccades_odd by merging with metadata_df\n",
        "    df_saccades_even = pd.merge(df_saccades_even, metadata_df[['trial_index_global', 'stimId']],\n",
        "                                left_on='Trial', right_on='trial_index_global', how='left').drop(columns='trial_index_global')\n",
        "    df_saccades_odd = pd.merge(df_saccades_odd, metadata_df[['trial_index_global', 'stimId']],\n",
        "                               left_on='Trial', right_on='trial_index_global', how='left').drop(columns='trial_index_global')\n",
        "\n",
        "else:\n",
        "    print(\"Warning: df_saccades not found or empty. Cannot create df_saccades_even and df_saccades_odd.\")\n",
        "    df_saccades_even = pd.DataFrame()\n",
        "    df_saccades_odd = pd.DataFrame()\n",
        "\n",
        "\n",
        "# 11) Save the results\n",
        "print(\"Saving outputs...\")\n",
        "np.save(f\"{output_prefix}Spike_count_even_sessions.npy\", Spike_count_even)\n",
        "np.save(f\"{output_prefix}Spike_count_odd_sessions.npy\", Spike_count_odd)\n",
        "np.save(f\"{output_prefix}Mean_eye_position_even_sessions.npy\", Mean_eye_position_even)\n",
        "np.save(f\"{output_prefix}Mean_eye_position_odd_sessions.npy\", Mean_eye_position_odd)\n",
        "np.save(f\"{output_prefix}Mean_pupil_size_even_sessions.npy\", Mean_pupil_size_even)\n",
        "np.save(f\"{output_prefix}Mean_pupil_size_odd_sessions.npy\", Mean_pupil_size_odd)\n",
        "\n",
        "# Save microsaccade-free outputs\n",
        "if 'df_saccades' in locals() and not df_saccades.empty:\n",
        "    np.save(f\"{output_prefix}Spike_count_even_sessions_ms_free.npy\", Spike_count_even_ms_free)\n",
        "    np.save(f\"{output_prefix}Spike_count_odd_sessions_ms_free.npy\", Spike_count_odd_ms_free)\n",
        "    np.save(f\"{output_prefix}Mean_eye_position_even_sessions_ms_free.npy\", Mean_eye_position_even_ms_free)\n",
        "    np.save(f\"{output_prefix}Mean_eye_position_odd_sessions_ms_free.npy\", Mean_eye_position_odd_ms_free)\n",
        "    np.save(f\"{output_prefix}Mean_pupil_size_even_sessions_ms_free.npy\", Mean_pupil_size_even_ms_free)\n",
        "    np.save(f\"{output_prefix}Mean_pupil_size_odd_sessions_ms_free.npy\", Mean_pupil_size_odd_ms_free)\n",
        "\n",
        "\n",
        "# Save df_saccades_even and df_saccades_odd as pickle files\n",
        "if not df_saccades_even.empty:\n",
        "    df_saccades_even.to_pickle(f\"{output_prefix}df_saccades_even.pkl\")\n",
        "    print(\"Saved df_saccades_even.pkl\")\n",
        "if not df_saccades_odd.empty:\n",
        "    df_saccades_odd.to_pickle(f\"{output_prefix}df_saccades_odd.pkl\")\n",
        "    print(\"Saved df_saccades_odd.pkl\")\n",
        "\n",
        "# save metadata\n",
        "np.save('valid_mask.npy', valid_mask)\n",
        "np.save('valid_mask_ms_free.npy', valid_mask_ms_free)\n",
        "print(\"Done! Session-split shifted arrays and mean signals saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package Final Outputs into a ZIP Archive\n",
        "\n",
        "This cell creates a compressed archive containing the final, downstream files\n"
      ],
      "metadata": {
        "id": "ZPirtgTlBJNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output archive name\n",
        "output_zip = \"final_outputs.zip\"\n",
        "\n",
        "# Regex patterns for files to include\n",
        "include_patterns = [\n",
        "    r\"^aggregated_spike_counts\\.npy$\",\n",
        "    r\"^aggregated_eye_pos\\.npy$\",\n",
        "    r\"^aggregated_pupil\\.npy$\",\n",
        "    r\"^recording_metadata\\.csv$\",\n",
        "    r\"^Spike_count_even_sessions\\.npy$\",\n",
        "    r\"^Spike_count_odd_sessions\\.npy$\",\n",
        "    r\"^Mean_eye_position_even_sessions\\.npy$\",\n",
        "    r\"^Mean_eye_position_odd_sessions\\.npy$\",\n",
        "    r\"^Mean_pupil_size_even_sessions\\.npy$\",\n",
        "    r\"^Mean_pupil_size_odd_sessions\\.npy$\",\n",
        "    r\"^saccade_data\\.pkl$\",\n",
        "    r\"^df_saccades_even\\.pkl$\",\n",
        "    r\"^df_saccades_odd\\.pkl$\",\n",
        "    r\"^Spike_count_even_sessions_ms_free\\.npy$\",\n",
        "    r\"^Spike_count_odd_sessions_ms_free\\.npy$\",\n",
        "    r\"^Mean_eye_position_even_sessions_ms_free\\.npy$\",\n",
        "    r\"^Mean_eye_position_odd_sessions_ms_free\\.npy$\",\n",
        "    r\"^Mean_pupil_size_even_sessions_ms_free\\.npy$\",\n",
        "    r\"^Mean_pupil_size_odd_sessions_ms_free\\.npy$\",\n",
        "    r\"^valid_mask\\.npy$\",\n",
        "    r\"^valid_mask_ms_free\\.npy$\",\n",
        "]\n",
        "\n",
        "# Compile regexes\n",
        "compiled = [re.compile(p) for p in include_patterns]\n",
        "\n",
        "with zipfile.ZipFile(output_zip, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for fname in os.listdir(\".\"):\n",
        "        # Only include files (no directories)\n",
        "        if not os.path.isfile(fname):\n",
        "            continue\n",
        "        # Check if filename matches any include pattern\n",
        "        if any(rx.match(fname) for rx in compiled):\n",
        "            print(f\"Adding {fname}\")\n",
        "            zf.write(fname, arcname=fname)\n",
        "\n",
        "print(f\"\\nCreated archive with final outputs: {output_zip}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8uZ3Y9yYjpI",
        "outputId": "76f5d40a-1822-4cd9-f77b-dc9907d82188"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding Spike_count_odd_sessions_ms_free.npy\n",
            "Adding saccade_data.pkl\n",
            "Adding Spike_count_even_sessions.npy\n",
            "Adding Mean_pupil_size_even_sessions_ms_free.npy\n",
            "Adding Mean_eye_position_even_sessions_ms_free.npy\n",
            "Adding Mean_eye_position_odd_sessions.npy\n",
            "Adding aggregated_pupil.npy\n",
            "Adding df_saccades_even.pkl\n",
            "Adding valid_mask_ms_free.npy\n",
            "Adding Spike_count_even_sessions_ms_free.npy\n",
            "Adding valid_mask.npy\n",
            "Adding Mean_eye_position_even_sessions.npy\n",
            "Adding Mean_pupil_size_odd_sessions_ms_free.npy\n",
            "Adding aggregated_eye_pos.npy\n",
            "Adding Mean_eye_position_odd_sessions_ms_free.npy\n",
            "Adding Mean_pupil_size_odd_sessions.npy\n",
            "Adding aggregated_spike_counts.npy\n",
            "Adding recording_metadata.csv\n",
            "Adding Spike_count_odd_sessions.npy\n",
            "Adding Mean_pupil_size_even_sessions.npy\n",
            "Adding df_saccades_odd.pkl\n",
            "\n",
            "Created archive with final outputs: final_outputs.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDyBYtBm0wwX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}