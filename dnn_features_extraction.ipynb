{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobellet/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex/blob/main/dnn_features_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "widget-instructions"
      },
      "source": [
        "# This notebook extracts the feature activation in the penultimate layer of the DNN processing either the original or the low-passed images. We recommend using the free GPU environements provided by Kaggle or Google colab or to use a computer with cuda-enabled GPUs or MPS (M1 chip or later) on mac.\n",
        "---\n",
        "###Please ensure you have downloaded the required data from Figshare before proceeding.\n",
        "\n",
        "**Google Colab:**\n",
        "\n",
        "\n",
        "\n",
        "*   Upload the data to your Google Drive.\n",
        "*   Select Google Drive in the widget below and mount your drive for the fastest access.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Kaggle:**\n",
        "\n",
        "*   Add the dataset to your notebook's Input directory (e.g., ../input/dataset-name).\n",
        "\n",
        "*   Select Kaggle Input in the widget below.\n",
        "\n",
        "**Local Computer:**\n",
        "\n",
        "*   Store the downloaded files in a folder on your machine.\n",
        "\n",
        "*   Select Local Storage in the widget below and paste the absolute path to that folder (e.g., /Users/name/data/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "widget-code"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DATA SETUP WIDGET (Universal: Colab, Kaggle & Local)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. CONFIGURATION\n",
        "REQUIRED_FILES = [\n",
        "    'high_variation_stimuli.zip',\n",
        "    'inpainted_images.zip'\n",
        "]\n",
        "DEST_DIR = Path(\"downloads\")\n",
        "DEST_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Detect Environment\n",
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
        "\n",
        "# --- UI Elements ---\n",
        "style = {'description_width': '120px'}\n",
        "layout_full = widgets.Layout(width='98%')\n",
        "\n",
        "header = widgets.HTML(\"<h2>ðŸ“‚ Data Import Manager</h2>\")\n",
        "\n",
        "# Determine options based on environment\n",
        "options = ['Local Upload']\n",
        "if IN_COLAB:\n",
        "    options = ['Google Drive', 'Local Upload']\n",
        "elif IN_KAGGLE:\n",
        "    options = ['Kaggle Input', 'Local Upload']\n",
        "else:\n",
        "    options = ['Local Storage', 'Local Upload']\n",
        "\n",
        "source_toggle = widgets.ToggleButtons(\n",
        "    options=options,\n",
        "    description='Select Source:',\n",
        "    button_style='info',\n",
        "    style=style\n",
        ")\n",
        "\n",
        "# Input for Paths\n",
        "path_input = widgets.Text(\n",
        "    value='/content/drive/MyDrive/vlPFC_Geometry' if IN_COLAB else '.',\n",
        "    placeholder='Enter absolute path to data folder...',\n",
        "    description='Data Path:',\n",
        "    disabled=False,\n",
        "    layout=layout_full,\n",
        "    style=style\n",
        ")\n",
        "\n",
        "btn_action = widgets.Button(\n",
        "    description='Start Import',\n",
        "    button_style='primary',\n",
        "    icon='download',\n",
        "    layout=widgets.Layout(width='200px')\n",
        ")\n",
        "\n",
        "out_log = widgets.Output(layout={'border': '1px solid #ddd', 'padding': '10px'})\n",
        "\n",
        "# --- Logic ---\n",
        "def update_ui(change):\n",
        "    if source_toggle.value == 'Local Upload':\n",
        "        path_input.layout.display = 'none'\n",
        "    else:\n",
        "        path_input.layout.display = 'flex'\n",
        "        if source_toggle.value == 'Google Drive':\n",
        "            path_input.description = 'Drive Path:'\n",
        "        elif source_toggle.value == 'Local Storage':\n",
        "            path_input.description = 'Local Path:'\n",
        "        elif source_toggle.value == 'Kaggle Input':\n",
        "            path_input.description = 'Input Path:'\n",
        "            path_input.value = '/kaggle/input/vlpfc-geometry'\n",
        "\n",
        "source_toggle.observe(update_ui, 'value')\n",
        "\n",
        "def on_click_action(b):\n",
        "    with out_log:\n",
        "        clear_output()\n",
        "        mode = source_toggle.value\n",
        "\n",
        "        if mode == 'Local Upload':\n",
        "            if IN_COLAB:\n",
        "                from google.colab import files\n",
        "                print(\"ðŸš€ Upload files now (select all required files):\")\n",
        "                uploaded = files.upload()\n",
        "                for name in uploaded:\n",
        "                    shutil.move(name, DEST_DIR / name)\n",
        "                print(\"\\nâœ… Upload complete.\")\n",
        "            else:\n",
        "                print(\"âš ï¸ Manual upload widget is Colab-only.\")\n",
        "                print(f\"ðŸ‘‰ Please manually copy files to: {DEST_DIR.absolute()}\")\n",
        "            return\n",
        "\n",
        "        src_path = Path(path_input.value)\n",
        "        if mode == 'Google Drive' and not src_path.exists():\n",
        "            from google.colab import drive\n",
        "            print(\"ðŸ”„ Mounting Google Drive...\")\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        if not src_path.exists():\n",
        "            print(f\"âŒ Error: Path not found: {src_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"ðŸ”Ž Scanning {src_path} for files...\")\n",
        "        for fname in REQUIRED_FILES:\n",
        "            dest = DEST_DIR / fname\n",
        "            if dest.exists():\n",
        "                print(f\"   â€¢ {fname}: Already exists (Skipping)\")\n",
        "                continue\n",
        "\n",
        "            found = list(src_path.rglob(fname))\n",
        "            if found:\n",
        "                print(f\"   â€¢ Copying {fname}...\", end=\" \")\n",
        "                shutil.copy(found[0], dest)\n",
        "                print(\"Done.\")\n",
        "            else:\n",
        "                print(f\"   âŒ {fname}: Not found in source path.\")\n",
        "\n",
        "        missing = [f for f in REQUIRED_FILES if not (DEST_DIR / f).exists()]\n",
        "        if not missing:\n",
        "            print(\"\\nðŸŽ‰ Success! All files are in 'downloads/'. Run the next cell.\")\n",
        "        else:\n",
        "            print(f\"\\nâš ï¸ Missing files: {missing}\")\n",
        "\n",
        "update_ui(None)\n",
        "btn_action.on_click(on_click_action)\n",
        "display(header, source_toggle, path_input, btn_action, out_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQiFYdIa0x0_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "IN_COLAB = False\n",
        "IN_KAGGLE = False\n",
        "try:\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        IN_COLAB = True\n",
        "except NameError:\n",
        "    pass\n",
        "if not IN_COLAB:\n",
        "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n",
        "        IN_KAGGLE = True\n",
        "\n",
        "\n",
        "# Determine the path to the repository based on the environment\n",
        "if IN_COLAB:\n",
        "    path_to_repo = '/content/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex'\n",
        "elif IN_KAGGLE:\n",
        "    path_to_repo = '/kaggle/working/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex'\n",
        "else:\n",
        "    # Environment where the .py file is in the root of the repo\n",
        "    path_to_repo = '.'\n",
        "\n",
        "if IN_COLAB or IN_KAGGLE:\n",
        "    !pip install -q timm open_clip_torch git+https://github.com/openai/CLIP.git \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "# Only clone if not already present\n",
        "if not os.path.exists(path_to_repo):\n",
        "    os.system(\"git clone https://github.com/jobellet/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex.git \" + path_to_repo)\n",
        "sys.path.append(path_to_repo)\n",
        "sys.path.append(os.path.join(path_to_repo, 'utils')) # Add the utils directory to sys.path\n",
        "\n",
        "\n",
        "\n",
        "from utils.extract_and_download_data import unzip\n",
        "from utils.image_processing import m_pathway_filter_gaussian\n",
        "\n",
        "\n",
        "import shutil\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from multiprocessing import cpu_count\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import clip\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import types\n",
        "\n",
        "def setup_and_download():\n",
        "    \"\"\"\n",
        "    Checks for existence of required data files in 'downloads/' and unzips them.\n",
        "    (Actual download is handled by the widget above)\n",
        "    \"\"\"\n",
        "    from utils.extract_and_download_data import unzip\n",
        "\n",
        "    DOWNLOAD_DIR = Path(\"downloads\")\n",
        "\n",
        "    # high_variation_stimuli.zip\n",
        "    zip_path_1 = DOWNLOAD_DIR / \"high_variation_stimuli.zip\"\n",
        "    if zip_path_1.exists():\n",
        "        unzip(str(zip_path_1), \"\") # Unzip to current dir\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: {zip_path_1} not found.\")\n",
        "\n",
        "    # inpainted_images.zip\n",
        "    zip_path_2 = DOWNLOAD_DIR / \"inpainted_images.zip\"\n",
        "    if zip_path_2.exists():\n",
        "        unzip(str(zip_path_2), \"inpainted_images\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: {zip_path_2} not found.\")\n",
        "# -----------------------------------------------------------------------------\n",
        "#  Main image filtering loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def filter_images():\n",
        "    \"\"\"\n",
        "    Applies a low-pass filter to the high-variation stimuli images.\n",
        "    \"\"\"\n",
        "    stimulus_folder = 'high_variation_stimuli'\n",
        "    output_folder   = 'high_variation_stimuli_lowpass'\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    img_files = sorted(glob(stimulus_folder+\"/*.png\"))\n",
        "    for img_path in tqdm(img_files):\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "        lp  = m_pathway_filter_gaussian(img)\n",
        "        # OpenCV expects 8â€‘ or 32â€‘bit depths for colour conversion â†’ cast\n",
        "        if lp.dtype != np.uint8:\n",
        "            lp = np.clip(lp, 0, 255).astype(np.uint8)\n",
        "        lp_rgb = cv2.cvtColor(lp, cv2.COLOR_GRAY2RGB)\n",
        "        cv2.imwrite(os.path.join(output_folder,os.path.split(img_path)[1]), lp_rgb)\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#  Feature extraction\n",
        "# -----------------------------------------------------------------------------\n",
        "def extract_features():\n",
        "    \"\"\"\n",
        "    Extracts penultimate layer features from various deep neural networks.\n",
        "    \"\"\"\n",
        "    os.system('pip install -q timm open_clip_torch git+https://github.com/openai/CLIP.git --extra-index-url https://download.pytorch.org/whl/cu118')\n",
        "\n",
        "    GPU  = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    IN_ROOT   = Path.cwd()\n",
        "    OUT_ROOT  = Path(\"deepNetFeatures\");  OUT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "    CONDITIONS = {\n",
        "        \"high_variation_original\":  \"high_variation_stimuli\",\n",
        "        \"high_variation_lowpass\":   \"high_variation_stimuli_lowpass\",\n",
        "        \"inpainted_images_original\":  \"inpainted_images\",\n",
        "    }\n",
        "\n",
        "    CKPT = Path(\"_tmp_ckpt\");  CKPT.mkdir(exist_ok=True)\n",
        "    os.environ.update(TORCH_HOME=str(CKPT), XDG_CACHE_HOME=str(CKPT))\n",
        "\n",
        "    SUP_TIMM = {\n",
        "        \"ViT_base_patch16_224\"            : (\"vit_base_patch16_224\",            \"head_drop\"),\n",
        "        \"DeiT_small_distilled_patch16_224\": (\"deit_small_distilled_patch16_224\",\"head\"),\n",
        "        \"Swin_base_patch4_window7_224\"    : (\"swin_base_patch4_window7_224\",    \"head.fc\"),\n",
        "        \"ConvNeXt_base_in22ft1k\"          : (\"convnext_base_in22ft1k\",          \"head.drop\"),\n",
        "        \"EfficientNet_B0\"                 : (\"efficientnet_b0\",                 \"global_pool.flatten\"),\n",
        "        \"MobileNetV3_small_100\"           : (\"mobilenetv3_small_100\",           \"flatten\"),\n",
        "        \"ViT_large_patch16_224\"           : (\"vit_large_patch16_224\",           \"head_drop\"),\n",
        "        \"DeiT3_small_patch16_224\"         : (\"deit3_small_patch16_224\",         \"head_drop\"),\n",
        "        \"Swin_large_patch4_window7_224\"   : (\"swin_large_patch4_window7_224\",   \"head.fc\"),\n",
        "        \"ConvNeXt_tiny_in22ft1k\"          : (\"convnext_tiny_in22ft1k\",          \"head.drop\"),\n",
        "        \"MobileNetV3_large_100\"           : (\"mobilenetv3_large_100\",           \"flatten\"),\n",
        "    }\n",
        "\n",
        "    SUP_TV = {\n",
        "        \"ResNet50\"      : (torchvision.models.resnet50,      \"avgpool\"),\n",
        "        \"ResNet101\"     : (torchvision.models.resnet101,     \"avgpool\"),\n",
        "        \"Inception_v3\"  : (torchvision.models.inception_v3,  \"dropout\"),\n",
        "    }\n",
        "\n",
        "    CLIP_MODELS = {\n",
        "        \"CLIP_ViT-B/32\": (\"ViT-B/32\", \"visual.ln_post\"),\n",
        "        \"CLIP_RN50\"    : (\"RN50\",     \"visual.attnpool\"),\n",
        "    }\n",
        "\n",
        "    OPENCLIP = {\n",
        "        \"OpenCLIP_ViT-B/32_openai\"  : (\"ViT-B-32\", \"openai\",             \"visual.ln_post\"),\n",
        "        \"OpenCLIP_ViT-B/32_laion2b\" : (\"ViT-B-32\", \"laion2b_s34b_b79k\",  \"visual.ln_post\"),\n",
        "        \"OpenCLIP_RN50_openai\"      : (\"RN50\",     \"openai\",             \"visual.attnpool\"),\n",
        "        \"OpenCLIP_RN101_openai\"     : (\"RN101\",    \"openai\",             \"visual.attnpool\"),\n",
        "    }\n",
        "\n",
        "    DINO_TIMM = {\n",
        "        \"ViT_S16_DINO\": (\"vit_small_patch16_224\", \"head_drop\"),\n",
        "        \"ViT_B16_DINO\": (\"vit_base_patch16_224\",  \"head_drop\"),\n",
        "    }\n",
        "    DINO_HUB = {\n",
        "        \"DINO_ResNet50\": (\"facebookresearch/dino:main\", \"dino_resnet50\"),\n",
        "    }\n",
        "\n",
        "    MODELS = {}\n",
        "    for k,v in SUP_TIMM.items():   MODELS[k] = dict(fam=\"sup_timm\", arch=v[0], pen=v[1])\n",
        "    for k,v in SUP_TV.items():     MODELS[k] = dict(fam=\"sup_tv\",   ctor=v[0], pen=v[1])\n",
        "    for k,v in CLIP_MODELS.items():MODELS[k] = dict(fam=\"clip\",     arch=v[0], pen=v[1])\n",
        "    for k,v in OPENCLIP.items():   MODELS[k] = dict(fam=\"openclip\", arch=v[0], weights=v[1], pen=v[2])\n",
        "    for k,v in DINO_TIMM.items():  MODELS[k] = dict(fam=\"dino_timm\",arch=v[0], pen=v[1])\n",
        "    for k,v in DINO_HUB.items():   MODELS[k] = dict(fam=\"dino_hub\", repo=v[0], entry=v[1])\n",
        "\n",
        "    def safe_name(name: str) -> str:\n",
        "        return name.replace(\"/\", \"_\")\n",
        "\n",
        "    TX_STD = T.Compose([T.Resize(256), T.CenterCrop(224),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "\n",
        "    def load_batch(paths, tx, dev):\n",
        "        imgs = [tx(Image.open(p).convert(\"RGB\")) for p in paths]\n",
        "        return torch.stack(imgs).to(dev)\n",
        "\n",
        "    def attach_hook(model, layer):\n",
        "        buf = {}\n",
        "        for n,m in model.named_modules():\n",
        "            if n==layer:\n",
        "                m.register_forward_hook(lambda _,__,o: buf.setdefault(\"x\", o))\n",
        "                return buf\n",
        "        raise RuntimeError(f\"layer {layer} not found\")\n",
        "\n",
        "    def make_fwd(model, store):\n",
        "        def fn(x):\n",
        "            store.clear()\n",
        "            _ = model(x)\n",
        "            return store[\"x\"]\n",
        "        return fn\n",
        "\n",
        "    BATCH = 32 if GPU else 4\n",
        "\n",
        "    for nick, spec in MODELS.items():\n",
        "        out_paths = [\n",
        "            OUT_ROOT / f\"{safe_name(nick)}_features_{cond}.pkl\"\n",
        "            for cond in CONDITIONS\n",
        "        ]\n",
        "        if all(p.exists() for p in out_paths):\n",
        "            print(f\"All pickle files for {nick} exist; skipping model load.\")\n",
        "            continue\n",
        "\n",
        "        fam = spec[\"fam\"]\n",
        "        run_dev = device\n",
        "        print(f\"\\n=== {nick}  ({fam}) on {run_dev} ===\")\n",
        "\n",
        "        try:\n",
        "            if fam == \"sup_timm\":\n",
        "                mdl = timm.create_model(spec[\"arch\"], pretrained=True).to(run_dev).eval()\n",
        "                buf = attach_hook(mdl, spec[\"pen\"]); fwd = make_fwd(mdl, buf); preprocess = TX_STD\n",
        "            elif fam == \"sup_tv\":\n",
        "                mdl = spec[\"ctor\"](pretrained=True).to(run_dev).eval()\n",
        "                buf = attach_hook(mdl, spec[\"pen\"]); fwd = make_fwd(mdl, buf); preprocess = TX_STD\n",
        "            elif fam == \"clip\":\n",
        "                mdl, preprocess = clip.load(spec[\"arch\"], device=run_dev, jit=False)\n",
        "                mdl.eval(); fwd = lambda x: mdl.encode_image(x)\n",
        "            elif fam == \"openclip\":\n",
        "                mdl, _, preprocess = open_clip.create_model_and_transforms(\n",
        "                    spec[\"arch\"], pretrained=spec[\"weights\"], device=run_dev)\n",
        "                mdl.eval(); fwd = lambda x: mdl.encode_image(x)\n",
        "            elif fam == \"dino_timm\":\n",
        "                mdl = timm.create_model(spec[\"arch\"], pretrained=True,\n",
        "                                        pretrained_cfg_overlay=dict(tag=\"dino\")).to(run_dev).eval()\n",
        "                fwd = mdl; preprocess = TX_STD\n",
        "            elif fam == \"dino_hub\":\n",
        "                utils_mod = sys.modules.get(\"utils\", types.ModuleType(\"utils\"))\n",
        "                def trunc_normal_(tensor, mean=0., std=1.):\n",
        "                    return torch.nn.init.trunc_normal_(tensor, mean=mean, std=std)\n",
        "                utils_mod.trunc_normal_ = trunc_normal_\n",
        "                sys.modules[\"utils\"] = utils_mod\n",
        "                mdl = torch.hub.load(spec[\"repo\"], spec[\"entry\"])\n",
        "                mdl.to(run_dev).eval()\n",
        "                fwd = mdl; preprocess = TX_STD\n",
        "            else:\n",
        "                raise RuntimeError(\"unexpected family\")\n",
        "\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"  !! could not load {nick}: {e}\")\n",
        "            shutil.rmtree(CKPT, ignore_errors=True); CKPT.mkdir(exist_ok=True)\n",
        "            continue\n",
        "\n",
        "        for cond, folder_name in CONDITIONS.items():\n",
        "            out_pkl = OUT_ROOT / f\"{safe_name(nick)}_features_{cond}.pkl\"\n",
        "            if os.path.exists(out_pkl):\n",
        "                print(f\"{out_pkl} already exists. Skipping ...\")\n",
        "                continue\n",
        "\n",
        "            folder = IN_ROOT / folder_name\n",
        "            if not folder.exists():\n",
        "                print(f\"Folder {folder} does not exist, skipping condition {cond}\")\n",
        "                continue\n",
        "\n",
        "            files  = sorted([p for p in folder.iterdir()\n",
        "                             if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\"}])\n",
        "            feats, names = [], []\n",
        "\n",
        "            for i in tqdm(range(0,len(files),BATCH), desc=f\"{nick} | {cond}\", leave=False):\n",
        "                batch_paths = files[i:i+BATCH]\n",
        "                x = load_batch(batch_paths, preprocess, run_dev)\n",
        "                with torch.no_grad():\n",
        "                    out = fwd(x).detach().cpu()\n",
        "                feats.append(out)\n",
        "                names += [p.stem for p in batch_paths]\n",
        "\n",
        "            if not feats:\n",
        "                print(f\"No features extracted for {nick} | {cond}\")\n",
        "                continue\n",
        "\n",
        "            feats = torch.cat(feats).numpy()\n",
        "\n",
        "            with open(out_pkl,\"wb\") as fh:\n",
        "                pickle.dump({\"penultimate\":feats, \"image_names\":names}, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            print(f\"  saved {out_pkl.name:45s} {feats.shape}\")\n",
        "\n",
        "        del mdl; torch.cuda.empty_cache()\n",
        "        shutil.rmtree(CKPT, ignore_errors=True); CKPT.mkdir(exist_ok=True)\n",
        "        print(\"  (cache cleared)\")\n",
        "\n",
        "    output_zip_name = 'deepNetFeatures'\n",
        "    shutil.make_archive(output_zip_name, 'zip', root_dir=OUT_ROOT, base_dir='.')\n",
        "    print(f\"\\nAll .pkl files zipped into {output_zip_name}.zip\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup_and_download()\n",
        "    filter_images()\n",
        "    extract_features()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98GyeJvn6KQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}