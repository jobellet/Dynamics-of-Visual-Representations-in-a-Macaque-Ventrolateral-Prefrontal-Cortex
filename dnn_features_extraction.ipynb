{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeLOJr4r0O03+gw6s6lvJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobellet/vlPFC_Visual_Geometry/blob/main/dnn_features_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQiFYdIa0x0_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "IN_COLAB = False\n",
        "IN_KAGGLE = False\n",
        "try:\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        IN_COLAB = True\n",
        "except NameError:\n",
        "    pass\n",
        "if not IN_COLAB:\n",
        "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n",
        "        IN_KAGGLE = True\n",
        "\n",
        "\n",
        "# Determine the path to the repository based on the environment\n",
        "if IN_COLAB:\n",
        "    path_to_repo = '/content/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex'\n",
        "elif IN_KAGGLE:\n",
        "    path_to_repo = '/kaggle/working/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex'\n",
        "else:\n",
        "    # Environment where the .py file is in the root of the repo\n",
        "    path_to_repo = '.'\n",
        "\n",
        "if IN_COLAB or IN_KAGGLE:\n",
        "    !pip install -q timm open_clip_torch git+https://github.com/openai/CLIP.git \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "# Only clone if not already present\n",
        "if not os.path.exists(path_to_repo):\n",
        "    os.system(\"git clone https://github.com/jobellet/Dynamics-of-Visual-Representations-in-a-Macaque-Ventrolateral-Prefrontal-Cortex.git \" + path_to_repo)\n",
        "sys.path.append(path_to_repo)\n",
        "sys.path.append(os.path.join(path_to_repo, 'utils')) # Add the utils directory to sys.path\n",
        "\n",
        "\n",
        "from utils.extract_and_download_data import download_files, unzip\n",
        "from utils.image_processing import m_pathway_filter_gaussian\n",
        "\n",
        "\n",
        "import shutil\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from multiprocessing import cpu_count\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import clip\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import types\n",
        "# -----------------------------------------------------------------------------\n",
        "#  Setup and data download\n",
        "# -----------------------------------------------------------------------------\n",
        "def setup_and_download():\n",
        "    \"\"\"\n",
        "    Clones the git repository, prompts for a private link token, and downloads\n",
        "    the required data files.\n",
        "    \"\"\"\n",
        "\n",
        "    from utils.extract_and_download_data import download_files, unzip\n",
        "\n",
        "    private_link = input(\"Please enter your private link token: \")\n",
        "\n",
        "    files_to_download = [\n",
        "        \"high_variation_stimuli.zip\",\n",
        "        \"inpainted_images.zip\"\n",
        "    ]\n",
        "    download_files(path_to_repo,files_to_download, private_link)\n",
        "\n",
        "    unzip(\"downloads/high_variation_stimuli.zip\", \"\")\n",
        "    unzip(\"downloads/inpainted_images.zip\", \"inpainted_images\")\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#  Main image filtering loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def filter_images():\n",
        "    \"\"\"\n",
        "    Applies a low-pass filter to the high-variation stimuli images.\n",
        "    \"\"\"\n",
        "    stimulus_folder = 'high_variation_stimuli'\n",
        "    output_folder   = 'high_variation_stimuli_lowpass'\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    img_files = sorted(glob(stimulus_folder+\"/*.png\"))\n",
        "    for img_path in tqdm(img_files):\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "        lp  = m_pathway_filter_gaussian(img)\n",
        "        # OpenCV expects 8‑ or 32‑bit depths for colour conversion → cast\n",
        "        if lp.dtype != np.uint8:\n",
        "            lp = np.clip(lp, 0, 255).astype(np.uint8)\n",
        "        lp_rgb = cv2.cvtColor(lp, cv2.COLOR_GRAY2RGB)\n",
        "        cv2.imwrite(os.path.join(output_folder,os.path.split(img_path)[1]), lp_rgb)\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#  Feature extraction\n",
        "# -----------------------------------------------------------------------------\n",
        "def extract_features():\n",
        "    \"\"\"\n",
        "    Extracts penultimate layer features from various deep neural networks.\n",
        "    \"\"\"\n",
        "    os.system('pip install -q timm open_clip_torch git+https://github.com/openai/CLIP.git --extra-index-url https://download.pytorch.org/whl/cu118')\n",
        "\n",
        "    GPU  = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    IN_ROOT   = Path.cwd()\n",
        "    OUT_ROOT  = Path(\"deepNetFeatures\");  OUT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "    CONDITIONS = {\n",
        "        \"high_variation_original\":  \"high_variation_stimuli\",\n",
        "        \"high_variation_lowpass\":   \"high_variation_stimuli_lowpass\",\n",
        "        \"inpainted_images_original\":  \"inpainted_images\",\n",
        "    }\n",
        "\n",
        "    CKPT = Path(\"_tmp_ckpt\");  CKPT.mkdir(exist_ok=True)\n",
        "    os.environ.update(TORCH_HOME=str(CKPT), XDG_CACHE_HOME=str(CKPT))\n",
        "\n",
        "    SUP_TIMM = {\n",
        "        \"ViT_base_patch16_224\"            : (\"vit_base_patch16_224\",            \"head_drop\"),\n",
        "        \"DeiT_small_distilled_patch16_224\": (\"deit_small_distilled_patch16_224\",\"head\"),\n",
        "        \"Swin_base_patch4_window7_224\"    : (\"swin_base_patch4_window7_224\",    \"head.fc\"),\n",
        "        \"ConvNeXt_base_in22ft1k\"          : (\"convnext_base_in22ft1k\",          \"head.drop\"),\n",
        "        \"EfficientNet_B0\"                 : (\"efficientnet_b0\",                 \"global_pool.flatten\"),\n",
        "        \"MobileNetV3_small_100\"           : (\"mobilenetv3_small_100\",           \"flatten\"),\n",
        "        \"ViT_large_patch16_224\"           : (\"vit_large_patch16_224\",           \"head_drop\"),\n",
        "        \"DeiT3_small_patch16_224\"         : (\"deit3_small_patch16_224\",         \"head_drop\"),\n",
        "        \"Swin_large_patch4_window7_224\"   : (\"swin_large_patch4_window7_224\",   \"head.fc\"),\n",
        "        \"ConvNeXt_tiny_in22ft1k\"          : (\"convnext_tiny_in22ft1k\",          \"head.drop\"),\n",
        "        \"MobileNetV3_large_100\"           : (\"mobilenetv3_large_100\",           \"flatten\"),\n",
        "    }\n",
        "\n",
        "    SUP_TV = {\n",
        "        \"ResNet50\"      : (torchvision.models.resnet50,      \"avgpool\"),\n",
        "        \"ResNet101\"     : (torchvision.models.resnet101,     \"avgpool\"),\n",
        "        \"Inception_v3\"  : (torchvision.models.inception_v3,  \"dropout\"),\n",
        "    }\n",
        "\n",
        "    CLIP_MODELS = {\n",
        "        \"CLIP_ViT-B/32\": (\"ViT-B/32\", \"visual.ln_post\"),\n",
        "        \"CLIP_RN50\"    : (\"RN50\",     \"visual.attnpool\"),\n",
        "    }\n",
        "\n",
        "    OPENCLIP = {\n",
        "        \"OpenCLIP_ViT-B/32_openai\"  : (\"ViT-B-32\", \"openai\",             \"visual.ln_post\"),\n",
        "        \"OpenCLIP_ViT-B/32_laion2b\" : (\"ViT-B-32\", \"laion2b_s34b_b79k\",  \"visual.ln_post\"),\n",
        "        \"OpenCLIP_RN50_openai\"      : (\"RN50\",     \"openai\",             \"visual.attnpool\"),\n",
        "        \"OpenCLIP_RN101_openai\"     : (\"RN101\",    \"openai\",             \"visual.attnpool\"),\n",
        "    }\n",
        "\n",
        "    DINO_TIMM = {\n",
        "        \"ViT_S16_DINO\": (\"vit_small_patch16_224\", \"head_drop\"),\n",
        "        \"ViT_B16_DINO\": (\"vit_base_patch16_224\",  \"head_drop\"),\n",
        "    }\n",
        "    DINO_HUB = {\n",
        "        \"DINO_ResNet50\": (\"facebookresearch/dino:main\", \"dino_resnet50\"),\n",
        "    }\n",
        "\n",
        "    MODELS = {}\n",
        "    for k,v in SUP_TIMM.items():   MODELS[k] = dict(fam=\"sup_timm\", arch=v[0], pen=v[1])\n",
        "    for k,v in SUP_TV.items():     MODELS[k] = dict(fam=\"sup_tv\",   ctor=v[0], pen=v[1])\n",
        "    for k,v in CLIP_MODELS.items():MODELS[k] = dict(fam=\"clip\",     arch=v[0], pen=v[1])\n",
        "    for k,v in OPENCLIP.items():   MODELS[k] = dict(fam=\"openclip\", arch=v[0], weights=v[1], pen=v[2])\n",
        "    for k,v in DINO_TIMM.items():  MODELS[k] = dict(fam=\"dino_timm\",arch=v[0], pen=v[1])\n",
        "    for k,v in DINO_HUB.items():   MODELS[k] = dict(fam=\"dino_hub\", repo=v[0], entry=v[1])\n",
        "\n",
        "    def safe_name(name: str) -> str:\n",
        "        return name.replace(\"/\", \"_\")\n",
        "\n",
        "    TX_STD = T.Compose([T.Resize(256), T.CenterCrop(224),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "\n",
        "    def load_batch(paths, tx, dev):\n",
        "        imgs = [tx(Image.open(p).convert(\"RGB\")) for p in paths]\n",
        "        return torch.stack(imgs).to(dev)\n",
        "\n",
        "    def attach_hook(model, layer):\n",
        "        buf = {}\n",
        "        for n,m in model.named_modules():\n",
        "            if n==layer:\n",
        "                m.register_forward_hook(lambda _,__,o: buf.setdefault(\"x\", o))\n",
        "                return buf\n",
        "        raise RuntimeError(f\"layer {layer} not found\")\n",
        "\n",
        "    def make_fwd(model, store):\n",
        "        def fn(x):\n",
        "            store.clear()\n",
        "            _ = model(x)\n",
        "            return store[\"x\"]\n",
        "        return fn\n",
        "\n",
        "    BATCH = 32 if GPU else 4\n",
        "\n",
        "    for nick, spec in MODELS.items():\n",
        "        out_paths = [\n",
        "            OUT_ROOT / f\"{safe_name(nick)}_features_{cond}.pkl\"\n",
        "            for cond in CONDITIONS\n",
        "        ]\n",
        "        if all(p.exists() for p in out_paths):\n",
        "            print(f\"All pickle files for {nick} exist; skipping model load.\")\n",
        "            continue\n",
        "\n",
        "        fam = spec[\"fam\"]\n",
        "        run_dev = device\n",
        "        print(f\"\\n=== {nick}  ({fam}) on {run_dev} ===\")\n",
        "\n",
        "        try:\n",
        "            if fam == \"sup_timm\":\n",
        "                mdl = timm.create_model(spec[\"arch\"], pretrained=True).to(run_dev).eval()\n",
        "                buf = attach_hook(mdl, spec[\"pen\"]); fwd = make_fwd(mdl, buf); preprocess = TX_STD\n",
        "            elif fam == \"sup_tv\":\n",
        "                mdl = spec[\"ctor\"](pretrained=True).to(run_dev).eval()\n",
        "                buf = attach_hook(mdl, spec[\"pen\"]); fwd = make_fwd(mdl, buf); preprocess = TX_STD\n",
        "            elif fam == \"clip\":\n",
        "                mdl, preprocess = clip.load(spec[\"arch\"], device=run_dev, jit=False)\n",
        "                mdl.eval(); fwd = lambda x: mdl.encode_image(x)\n",
        "            elif fam == \"openclip\":\n",
        "                mdl, _, preprocess = open_clip.create_model_and_transforms(\n",
        "                    spec[\"arch\"], pretrained=spec[\"weights\"], device=run_dev)\n",
        "                mdl.eval(); fwd = lambda x: mdl.encode_image(x)\n",
        "            elif fam == \"dino_timm\":\n",
        "                mdl = timm.create_model(spec[\"arch\"], pretrained=True,\n",
        "                                        pretrained_cfg_overlay=dict(tag=\"dino\")).to(run_dev).eval()\n",
        "                fwd = mdl; preprocess = TX_STD\n",
        "            elif fam == \"dino_hub\":\n",
        "                utils_mod = sys.modules.get(\"utils\", types.ModuleType(\"utils\"))\n",
        "                def trunc_normal_(tensor, mean=0., std=1.):\n",
        "                    return torch.nn.init.trunc_normal_(tensor, mean=mean, std=std)\n",
        "                utils_mod.trunc_normal_ = trunc_normal_\n",
        "                sys.modules[\"utils\"] = utils_mod\n",
        "                mdl = torch.hub.load(spec[\"repo\"], spec[\"entry\"])\n",
        "                mdl.to(run_dev).eval()\n",
        "                fwd = mdl; preprocess = TX_STD\n",
        "            else:\n",
        "                raise RuntimeError(\"unexpected family\")\n",
        "\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"  !! could not load {nick}: {e}\")\n",
        "            shutil.rmtree(CKPT, ignore_errors=True); CKPT.mkdir(exist_ok=True)\n",
        "            continue\n",
        "\n",
        "        for cond, folder_name in CONDITIONS.items():\n",
        "            out_pkl = OUT_ROOT / f\"{safe_name(nick)}_features_{cond}.pkl\"\n",
        "            if os.path.exists(out_pkl):\n",
        "                print(f\"{out_pkl} already exists. Skipping ...\")\n",
        "                continue\n",
        "\n",
        "            folder = IN_ROOT / folder_name\n",
        "            if not folder.exists():\n",
        "                print(f\"Folder {folder} does not exist, skipping condition {cond}\")\n",
        "                continue\n",
        "\n",
        "            files  = sorted([p for p in folder.iterdir()\n",
        "                             if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\"}])\n",
        "            feats, names = [], []\n",
        "\n",
        "            for i in tqdm(range(0,len(files),BATCH), desc=f\"{nick} | {cond}\", leave=False):\n",
        "                batch_paths = files[i:i+BATCH]\n",
        "                x = load_batch(batch_paths, preprocess, run_dev)\n",
        "                with torch.no_grad():\n",
        "                    out = fwd(x).detach().cpu()\n",
        "                feats.append(out)\n",
        "                names += [p.stem for p in batch_paths]\n",
        "\n",
        "            if not feats:\n",
        "                print(f\"No features extracted for {nick} | {cond}\")\n",
        "                continue\n",
        "\n",
        "            feats = torch.cat(feats).numpy()\n",
        "\n",
        "            with open(out_pkl,\"wb\") as fh:\n",
        "                pickle.dump({\"penultimate\":feats, \"image_names\":names}, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            print(f\"  saved {out_pkl.name:45s} {feats.shape}\")\n",
        "\n",
        "        del mdl; torch.cuda.empty_cache()\n",
        "        shutil.rmtree(CKPT, ignore_errors=True); CKPT.mkdir(exist_ok=True)\n",
        "        print(\"  (cache cleared)\")\n",
        "\n",
        "    output_zip_name = 'deepNetFeatures'\n",
        "    shutil.make_archive(output_zip_name, 'zip', root_dir=OUT_ROOT, base_dir='.')\n",
        "    print(f\"\\nAll .pkl files zipped into {output_zip_name}.zip\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup_and_download()\n",
        "    filter_images()\n",
        "    extract_features()"
      ]
    }
  ]
}